{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import urllib3 as ur\n",
    "import urllib.request as ur\n",
    "import os.path\n",
    "import zipfile\n",
    "import tinys3\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import logging as log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python       \n",
    "merged_dataframe=pd.DataFrame()\n",
    "df_list_global=list()\n",
    "class GetData:\n",
    "   \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "    def create_directory(self,path):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    def setDataFrame(self, df):\n",
    "        merged_dataframe = df\n",
    "        \n",
    "    def getDataFrame(self):\n",
    "        return merged_dataframe\n",
    "    \n",
    "    def setDataFrameList(self, list_of_df):\n",
    "        \n",
    "        df_list_global = list_of_df\n",
    "      \n",
    "        \n",
    "    def getDataFrameList(self):\n",
    "        return df_list_global\n",
    "    \n",
    "    def maybe_download(self, url_list, year):\n",
    "        \n",
    "        df_list=['df1','df2','df3','df4','df5','df6','df7','df8','df9','df10','df11','df12']\n",
    "        year=str(year)\n",
    "        count=0\n",
    "        print(\"Downloading data for all the months\")\n",
    "        log.info(\"Downloading data for all the months\")\n",
    "        \n",
    "        for i in url_list:\n",
    "\n",
    "            #fetching the zip file name from the URL\n",
    "            file_name=i.split(\"/\")\n",
    "           \n",
    "\n",
    "            #Downloading data if not already present in the cache\n",
    "            if(os.path.exists(\"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8])):\n",
    "                print(\"Data for \",file_name[8],\" is already present, pulling it from cache\")\n",
    "                \n",
    "\n",
    "            else:\n",
    "                #pbar = ProgressBar(widgets=[Percentage(), Bar()])\n",
    "                ur.urlretrieve(i, \"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8])\n",
    "                #ur.urlretrieve(i, \"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8], reporthook)\n",
    "                print(\"Data for \",file_name[8],\"not present in cache. Downloading data\")\n",
    "                \n",
    "            \n",
    "            #unzip the file and fetch the csv file\n",
    "            zf = zipfile.ZipFile(\"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8]) \n",
    "            csv_file_name=file_name[8].replace(\"zip\", \"csv\")\n",
    "            zf_file=zf.open(csv_file_name)\n",
    "            \n",
    "            \n",
    "            #create a dataframe from the csv and append it to the list of dataframe\n",
    "            df_list[count]=pd.read_csv(zf_file)\n",
    "           \n",
    "            count=count+1 \n",
    "        \n",
    "        print(\"All the files are downloaded and unzipped\")\n",
    "        log.info(\"All the files are downloaded and unzipped\")\n",
    "        print(\"Creating a dataframe from the csv and appending it to the list of dataframe\")\n",
    "        log.info(\"Creating a dataframe from the csv and appending it to the list of dataframe\")\n",
    "        \n",
    "        self.setDataFrameList(df_list)\n",
    "        log.info(\"Merging the dataframe\")\n",
    "        #merging the data into one dataframe\n",
    "        merged_dataframe=pd.concat([df_list[0],df_list[1],df_list[2],df_list[3],df_list[4],df_list[5],df_list[6],df_list[7],df_list[8],df_list[9],df_list[10],df_list[11]], ignore_index=True)\n",
    "        self.setDataFrame(merged_dataframe)\n",
    "        return merged_dataframe\n",
    "    \n",
    "\n",
    "    def generate_url(self, year):\n",
    "        log.info('In generate URL method')\n",
    "        print(\"Generating the URL's for the year\")\n",
    "        log.info(\"Generating the URL's for the year\")\n",
    "        url_list=list()\n",
    "        #generate the url for fetching the log files for every month's first day\n",
    "        number_of_months=1\n",
    "\n",
    "        while number_of_months < 13:\n",
    "            #find the quarter for the month\n",
    "            if number_of_months >= 1 and number_of_months < 4:\n",
    "                quarter=\"Qtr1\"\n",
    "            elif(number_of_months >= 4 and number_of_months < 7):\n",
    "                quarter=\"Qtr2\"\n",
    "            elif(number_of_months >= 7 and number_of_months < 10):\n",
    "                quarter=\"Qtr3\"\n",
    "            elif(number_of_months >= 10 and number_of_months < 13):\n",
    "                quarter=\"Qtr4\"\n",
    "\n",
    "            if(number_of_months <10):\n",
    "                url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+'%02d' % number_of_months+\"01.zip\"\n",
    "\n",
    "            else:\n",
    "                url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+str(number_of_months)+\"01.zip\"\n",
    "            \n",
    "            \n",
    "            url_list.append(url)\n",
    "            number_of_months=number_of_months+1\n",
    "\n",
    "        return self.maybe_download(url_list,year)\n",
    "        \n",
    "    def fetch_year(self):\n",
    "        year1 = input('Enter the year (eg : 2003) for which you need to fetch the log files. Note: Data available for years 2003 through 2016 only.')\n",
    "\n",
    "        try:\n",
    "            year=int(year1)\n",
    "            if(year >= 2003 and year <= 2016):\n",
    "                #calling the function to generate dynamic URL\n",
    "                self.create_directory(\"Part_2_log_datasets_trial/\"+str(year)+\"/\")\n",
    "                log.basicConfig(filename='Part_2_log_datasets_trial/EDGAR_LogFileDataset_LogFile.log', level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "                return self.generate_url(year)\n",
    "            else:\n",
    "                print(\"EDGAR log files are available for years 2003-2016. Kindly enter a year within this range\")\n",
    "                self.fetch_year()\n",
    "                \n",
    "        except Exception:\n",
    "            print(\"Invalid input. Please try again\")\n",
    "            self.fetch_year()\n",
    "        #fetch the year for which the user wants logs\n",
    "       \n",
    "        log.info('Start of program')    \n",
    "    \n",
    "                \n",
    "    def create_zip_folder(self,path):\n",
    "        zipfolder_name=path+'.zip'\n",
    "        zf = zipfile.ZipFile(zipfolder_name, \"w\")\n",
    "        for dirname, subdirs, files in os.walk(path):\n",
    "            zf.write(dirname)\n",
    "            for filename in files:\n",
    "                zf.write(os.path.join(dirname, filename))\n",
    "        zf.close()\n",
    "    \n",
    "        \n",
    "        \n",
    "    def upload_zip_to_s3(self,filetoupload):\n",
    "        print(\"Upload to s3\")\n",
    "        S3_ACCESS_KEY= input(\"Enter S3_ACCESS_KEY : \")\n",
    "        S3_SECRET_KEY =  input(\"Enter S3_SECRET_KEY : \")\n",
    "        \n",
    "\n",
    "        try:\n",
    "            conn = tinys3.Connection(S3_ACCESS_KEY,S3_SECRET_KEY)\n",
    "            bucket = input(\"Enter BUCKET_NAME : \")\n",
    "            f = open(filetoupload,'rb')\n",
    "#             print(\"this is f\",f)\n",
    "#             print(\"this is file to upload\",filetoupload)\n",
    "#             print(\"this is bucket\",bucket)\n",
    "            conn.upload(filetoupload,f,bucket)\n",
    "            print(\"Upload to s3 successfull. Proceeding to Analysis\")\n",
    "           \n",
    "        except Exception:\n",
    "            print(\"INVALID keys\")\n",
    "            choice = input(\"Proceed without uploading to s3? Y/N : (Select N to try again)\")\n",
    "            if(choice == \"Y\" or choice == \"y\"):\n",
    "                print(\"Folder not uploaded to S3. Proceeding to Analysis \")\n",
    "\n",
    "            elif(choice == \"N\" or choice == \"n\"):\n",
    "                self.upload_zip_to_s3(filetoupload)\n",
    "                \n",
    "            else:\n",
    "                print(\"Invalid input. Try again.\")\n",
    "                self.upload_zip_to_s3(filetoupload)\n",
    "                \n",
    "           \n",
    "        \n",
    "get_data_obj=GetData()\n",
    "merged_dataframe=get_data_obj.fetch_year()\n",
    "#fetch the year for which the user wants logs\n",
    "#year = input('Enter the year for which you need to fetch the log files: ')\n",
    "#calling the function to generate dynamic URL\n",
    "\n",
    "#df=get_data_obj.generate_url(year)\n",
    "\n",
    "\n",
    "class Process_and_analyse_data():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "    \n",
    "    def format_dataframe_columns(self):\n",
    "        #convert all the integer column in int format\n",
    "        log.info(\"Data fetched, started cleaning\")\n",
    "        print(\"Data fetched, started cleaning\")\n",
    "        df['zone'] = df['zone'].astype('int')\n",
    "        df['cik'] = df['cik'].astype('int')\n",
    "        df['code'] = df['code'].astype('int')\n",
    "        df['idx']=df['idx'].astype('int')\n",
    "        df['norefer']=df['norefer'].astype('int')\n",
    "        df['noagent']=df['noagent'].astype('int')\n",
    "        df['find']=df['find'].astype('int')\n",
    "        df['crawler']=df['crawler'].astype('int')\n",
    "        \n",
    "        #replacing empty strings with NaN \n",
    "        df.replace(r'\\s+', np.nan, regex=True)\n",
    "        log.info(\"Formatted the columns of the dataframe\")\n",
    "        print(\"Formatted the columns of the dataframe\")\n",
    "        self.handle_nan_values()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def handle_nan_values(self):\n",
    "        \n",
    "        #replace all ip column NaN value by a default ip address \n",
    "        df[\"ip\"].fillna(\"255.255.255.255\", inplace=True)\n",
    "\n",
    "        #perform forward fill to replace NaN values by fetching the next valid value\n",
    "        df[\"date\"].fillna(method='ffill')\n",
    "\n",
    "        #perform backward fill to replace NaN values by backpropagating and fetching the previous valid value\n",
    "        df[\"time\"].fillna(method='bfill')\n",
    "\n",
    "        #replace all zone column NaN values by 'Not Available' extension\n",
    "        df[\"zone\"].fillna(\"Not Available\", inplace=True)\n",
    "\n",
    "        #replace all extension column NaN values by default extension\n",
    "        df[\"extention\"].fillna(\"-index.htm\", inplace=True)\n",
    "\n",
    "        #replace all size column NaN values by 0 and convert the column into integer \n",
    "        df[\"size\"].fillna(0, inplace=True)\n",
    "        df['size'] = df['size'].astype('int')\n",
    "\n",
    "        #replace all user agent column NaN values by the default value 1 (no user agent)\n",
    "        df[\"noagent\"].fillna(\"Not Applicable\", inplace=True)\n",
    "\n",
    "        #replace all find column NaN values by the default value 0 (no character strings found)\n",
    "        df[\"find\"].fillna(0, inplace=True)\n",
    "\n",
    "        #replace all broser column NaN values by a string\n",
    "        df[\"browser\"].fillna(\"Not Available\", inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if the value in idx column is missing, check the value of the extension column, if its \"-index.html\" set the column's value 1 else 0\n",
    "        count=0\n",
    "        for i in df['idx']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df['extension'][count]==\"-index.htm\"):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            count=count+1\n",
    "\n",
    "        # if the value of norefer column is missing, check the value of the find column, if it is 0, set the value 1, else it set it 0\n",
    "        counter=0\n",
    "        for i in df['norefer']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df[\"find\"][counter]==0):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            counter=counter+1\n",
    "\n",
    "        # if the value of crawler is missing, check the value of the code, if it is 404 set it as 1 else 0\n",
    "        count_position=0\n",
    "        for i in df['crawler']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df[\"code\"][count_position]==404):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            count_position=count_position+1\n",
    "        log.info(\"Replacing NaN values with appropriate replacement\")\n",
    "        log.info(\"Handling missing values completed\")\n",
    "        print(\"Handling missing values completed\")\n",
    "        \n",
    "        log.info(\"Exporting merged dataframe to local system\")\n",
    "        print(\"Exporting merged dataframe to local system\")\n",
    "        df.to_csv(\"Part_2_log_datasets_trial/merged_dataframe.csv\")\n",
    "        log.info(\"Merged dataframe exported\")\n",
    "        print(\"Merged dataframe exported\")\n",
    "        \n",
    "        merged_dataframe\n",
    "        self.identify_cik_accession_number_anomaly()\n",
    "    \n",
    "        \n",
    "    def identify_cik_accession_number_anomaly(self):\n",
    "        #this operation requires a large amount of time for computaton, thus we are performing this on a subset of data\n",
    "        small_df=df.head(25)\n",
    "        #insert a column to check CIK, Accession number discripancy\n",
    "        small_df.insert(6, \"CIK_Accession_Anamoly_Flag\", \"N\")\n",
    "                \n",
    "        #check if CIK and Accession number match. The Accession number is divided into three parts, CIK-Year-Number_of_filings_listed.\n",
    "        #the first part i.e the CIK must match with the CIK column. If not, there exists an anomaly\n",
    "\n",
    "        count=0;\n",
    "        print(\"Creating CIK_Accession_Anomaly_Flag column to check anomaly\")\n",
    "        log.info(\"Creating CIK_Accession_Anomaly_Flag column to check anomaly\")\n",
    "        \n",
    "        for i in small_df['accession']:\n",
    "            #fetch the CIK number from the accession number and convert it into integer\n",
    "            list_of_fetched_cik_from_accession=[(int(i.split(\"-\")[0]))]\n",
    "\n",
    "            #check if the CIK number from the column and CIK number fetched from the accession number are equal\n",
    "            if(small_df['cik'][count]!=list_of_fetched_cik_from_accession):\n",
    "                small_df['CIK_Accession_Anamoly_Flag'][count]=\"Y\"\n",
    "\n",
    "            count=count+1\n",
    "        log.info(\"CIK Accession Anomaly flag computed\")\n",
    "        print(\"CIK Accession Anomaly flag computed\")\n",
    "        print(small_df)\n",
    "        self.get_file_name_from_extension()\n",
    "        \n",
    "    def get_file_name_from_extension(self):\n",
    "        #this operation requires a large amount of time for computaton, thus we are performing this on a subset of data\n",
    "        small_df=df.head(25)\n",
    "        small_df.insert(7, \"filename\", \"\")\n",
    "        print(\"Creating filename column\")\n",
    "        log.info(\"Creating filename column\")\n",
    "        #Extension rule: if the file name is missing and only the file extension is present, then the file name is document accession number\n",
    "        count=0\n",
    "        for i in small_df[\"extention\"]:\n",
    "            if(i==\".txt\"):\n",
    "                # if the value in extension is only .txt, fetch the accession number and append accession number to .txt\n",
    "                #list_of_fetched_cik_from_accession=int(((df2[\"accession\"].str.split(\"-\")[count])[0]))\n",
    "                #print((df[\"accession\"]).astype(str))\n",
    "                #list_of_fetched_cik_from_accession=int(df[\"accession\"])\n",
    "                small_df[\"filename\"][count]=(small_df[\"accession\"][count])+\".txt\" \n",
    "            else:\n",
    "                small_df[\"filename\"][count]=i\n",
    "            count=count+1\n",
    "        print(\"Filename column created\")\n",
    "        print(small_df)\n",
    "        log.info(\"Filename column created\")\n",
    "        \n",
    "get_data_obj=GetData()\n",
    "df=get_data_obj.getDataFrame()\n",
    "df_list=get_data_obj.getDataFrameList()\n",
    "process_data_obj=Process_and_analyse_data()\n",
    "process_data_obj.format_dataframe_columns()\n",
    "\n",
    "log.info(\"Zipping the folder for loading in S3\")\n",
    "get_data_obj.create_zip_folder(\"Part_2_log_datasets_trial\")\n",
    "get_data_obj.upload_zip_to_s3(\"Part_2_log_datasets_trial.zip\")\n",
    "log.info(\"Data zipped and loaded on S3\")\n",
    "print(\"Data zipped and loaded on S3\")\n",
    "log.info(\"Pipeline completed!!\")\n",
    "log.info(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Starting Analysis\")\n",
    "combined_df = pd.read_csv(\"Part_2_log_datasets_trial/merged_dataframe.csv\") #  pass your 12 month combined csv here\n",
    "# group by cik and date and get count of ciks for a date   \n",
    "temp_df=combined_df.groupby(['cik','date'])['cik'].count()\n",
    "temp_df.head()\n",
    "\n",
    "# convert group by result into a frame\n",
    "\n",
    "grouped_frame = pd.DataFrame(temp_df.reset_index(name = \"hit_count\"))\n",
    "\n",
    "print(\"Grouping by CIK and Date and getting count for each CIK  \")\n",
    "print(grouped_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Monitor change in hit count\n",
    "# get no of ips(hit count) for CIK per month(only day 1 representing a monthhas been used)\n",
    "# if the change is more than 1000 % select that CIK and make a new dataframe for it\n",
    "print(\"Monitoring the change in hit count\")\n",
    "def get_percent_change(curr, prev):\n",
    "        change_in_perc = ((curr - prev)/prev ) * 100\n",
    "        return change_in_perc\n",
    "\n",
    "count = 0\n",
    "analysis_df = pd.DataFrame()\n",
    "frame_count = 0\n",
    "for row in grouped_frame['cik']:\n",
    "    current_cik = grouped_frame['cik'][count]\n",
    "    current_hit_count = grouped_frame['hit_count'][count]\n",
    "    current_date = grouped_frame['date'][count]\n",
    "    if(count >= 1):\n",
    "        if(current_cik == grouped_frame['cik'][count-1]):\n",
    "            change_in_count = current_hit_count - grouped_frame['hit_count'][count-1] \n",
    "            change_in_perc = get_percent_change(current_hit_count,grouped_frame['hit_count'][count-1])\n",
    "            \n",
    "            if(change_in_perc >= 1000 ): ## decide on threshold\n",
    "                analysis_df.loc[frame_count, 'cik'] = current_cik\n",
    "                analysis_df.loc[frame_count, 'date'] = current_date\n",
    "                analysis_df.loc[frame_count, 'current count'] = current_hit_count\n",
    "                analysis_df.loc[frame_count, 'previous count'] = grouped_frame['cik'][count-1]\n",
    "                analysis_df.loc[frame_count, 'change in %'] = change_in_perc\n",
    "                frame_count += 1\n",
    "                #print(current_cik ,\" changed by\",change_in_perc,\" % on \",current_date)\n",
    "                \n",
    "    count +=1\n",
    "    \n",
    "print(analysis_df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
