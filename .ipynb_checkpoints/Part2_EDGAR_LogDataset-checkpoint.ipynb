{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import urllib3 as ur\n",
    "import urllib.request as ur\n",
    "import os.path\n",
    "import zipfile\n",
    "import tinys3\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import logging as log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the year for which you need to fetch the log files: 2003\n",
      "Generating the URL's for the year\n",
      "Downloading data for all the months\n",
      "Data for  log20030101.zip  is already present, pulling it from cache\n",
      "Data for  log20030201.zip  is already present, pulling it from cache\n",
      "Data for  log20030301.zip  is already present, pulling it from cache\n",
      "Data for  log20030401.zip  is already present, pulling it from cache\n",
      "Data for  log20030501.zip  is already present, pulling it from cache\n",
      "Data for  log20030601.zip  is already present, pulling it from cache\n",
      "Data for  log20030701.zip  is already present, pulling it from cache\n",
      "Data for  log20030801.zip  is already present, pulling it from cache\n",
      "Data for  log20030901.zip  is already present, pulling it from cache\n",
      "Data for  log20031001.zip  is already present, pulling it from cache\n",
      "Data for  log20031101.zip  is already present, pulling it from cache\n",
      "Data for  log20031201.zip  is already present, pulling it from cache\n",
      "All the files are downloaded and unzipped\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data fetched, started cleaning\n",
      "Formatted the columns of the dataframe\n",
      "Handling missing values completed\n",
      "Exporting merged dataframe to local system\n",
      "Merged dataframe exported\n",
      "Creating CIK_Accession_Anomaly_Flag column to check anomaly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:324: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:288: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIK Accession Anomaly flag computed\n",
      "Creating filename column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:348: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:330: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:346: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename column created\n",
      "Enter S3_ACCESS_KEY : AKIAJID52SF663DJKYWQ\n",
      "Enter S3_SECRET_KEY : 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\n",
      "Enter BUCKET_NAME : edgar-log\n",
      "this is f <_io.BufferedReader name='Part_2_log_datasets_trial.zip'>\n",
      "this is file to upload Part_2_log_datasets_trial.zip\n",
      "this is bucket edgar-log\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    904\u001b[0m                     \u001b[0mdatablock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatablock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatablock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m                 )\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    642\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 643\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0mretries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    904\u001b[0m                     \u001b[0mdatablock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatablock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatablock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7d14bea30638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Zipping the folder for loading in S3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0mget_data_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_zip_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Part_2_log_datasets_trial\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m \u001b[0mget_data_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload_zip_to_s3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Part_2_log_datasets_trial.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data zipped and loaded on S3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data zipped and loaded on S3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7d14bea30638>\u001b[0m in \u001b[0;36mupload_zip_to_s3\u001b[0;34m(self, filetoupload)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"this is file to upload\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfiletoupload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"this is bucket\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbucket\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiletoupload\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbucket\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Upload to s3 successfull\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tinys3\\connection.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(self, key, local_file, bucket, expires, content_type, public, headers, rewind, close)\u001b[0m\n\u001b[1;32m    169\u001b[0m                           \u001b[0mpublic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpublic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrewind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                           close=close)\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     def copy(self, from_key, from_bucket, to_key, to_bucket=None,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tinys3\\connection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \"\"\"\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhead_bucket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tinys3\\connection.py\u001b[0m in \u001b[0;36m_handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \"\"\"\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tinys3\\request_factory.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m                                    \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                                    \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                                    auth=self.auth)\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(url, data, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \"\"\"\n\u001b[1;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'put'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    486\u001b[0m         }\n\u001b[1;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python       \n",
    "merged_dataframe=pd.DataFrame()\n",
    "df_list_global=list()\n",
    "class GetData:\n",
    "   \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "    def create_directory(self,path):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    def setDataFrame(self, df):\n",
    "        merged_dataframe = df\n",
    "        \n",
    "    def getDataFrame(self):\n",
    "        return merged_dataframe\n",
    "    \n",
    "    def setDataFrameList(self, list_of_df):\n",
    "        \n",
    "        df_list_global = list_of_df\n",
    "      \n",
    "        \n",
    "    def getDataFrameList(self):\n",
    "        return df_list_global\n",
    "    \n",
    "    def maybe_download(self, url_list, year):\n",
    "        \n",
    "        df_list=['df1','df2','df3','df4','df5','df6','df7','df8','df9','df10','df11','df12']\n",
    "        year=str(year)\n",
    "        count=0\n",
    "        print(\"Downloading data for all the months\")\n",
    "        log.info(\"Downloading data for all the months\")\n",
    "        \n",
    "        for i in url_list:\n",
    "\n",
    "            #fetching the zip file name from the URL\n",
    "            file_name=i.split(\"/\")\n",
    "           \n",
    "\n",
    "            #Downloading data if not already present in the cache\n",
    "            if(os.path.exists(\"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8])):\n",
    "                print(\"Data for \",file_name[8],\" is already present, pulling it from cache\")\n",
    "                \n",
    "\n",
    "            else:\n",
    "                #pbar = ProgressBar(widgets=[Percentage(), Bar()])\n",
    "                ur.urlretrieve(i, \"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8])\n",
    "                #ur.urlretrieve(i, \"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8], reporthook)\n",
    "                print(\"Data for \",file_name[8],\"not present in cache. Downloading data\")\n",
    "                \n",
    "            \n",
    "            #unzip the file and fetch the csv file\n",
    "            zf = zipfile.ZipFile(\"Part_2_log_datasets_trial/\"+year+\"/\"+file_name[8]) \n",
    "            csv_file_name=file_name[8].replace(\"zip\", \"csv\")\n",
    "            zf_file=zf.open(csv_file_name)\n",
    "            \n",
    "            \n",
    "            #create a dataframe from the csv and append it to the list of dataframe\n",
    "            df_list[count]=pd.read_csv(zf_file)\n",
    "           \n",
    "            count=count+1 \n",
    "        \n",
    "        print(\"All the files are downloaded and unzipped\")\n",
    "        log.info(\"All the files are downloaded and unzipped\")\n",
    "        print(\"Creating a dataframe from the csv and appending it to the list of dataframe\")\n",
    "        log.info(\"Creating a dataframe from the csv and appending it to the list of dataframe\")\n",
    "        \n",
    "        self.setDataFrameList(df_list)\n",
    "        log.info(\"Merging the dataframe\")\n",
    "        #merging the data into one dataframe\n",
    "        merged_dataframe=pd.concat([df_list[0],df_list[1],df_list[2],df_list[3],df_list[4],df_list[5],df_list[6],df_list[7],df_list[8],df_list[9],df_list[10],df_list[11]], ignore_index=True)\n",
    "        self.setDataFrame(merged_dataframe)\n",
    "        return merged_dataframe\n",
    "    \n",
    "\n",
    "    def generate_url(self, year):\n",
    "        log.info('In generate URL method')\n",
    "        print(\"Generating the URL's for the year\")\n",
    "        log.info(\"Generating the URL's for the year\")\n",
    "        url_list=list()\n",
    "        #generate the url for fetching the log files for every month's first day\n",
    "        number_of_months=1\n",
    "\n",
    "        while number_of_months < 13:\n",
    "            #find the quarter for the month\n",
    "            if number_of_months >= 1 and number_of_months < 4:\n",
    "                quarter=\"Qtr1\"\n",
    "            elif(number_of_months >= 4 and number_of_months < 7):\n",
    "                quarter=\"Qtr2\"\n",
    "            elif(number_of_months >= 7 and number_of_months < 10):\n",
    "                quarter=\"Qtr3\"\n",
    "            elif(number_of_months >= 10 and number_of_months < 13):\n",
    "                quarter=\"Qtr4\"\n",
    "\n",
    "            if(number_of_months <10):\n",
    "                url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+'%02d' % number_of_months+\"01.zip\"\n",
    "\n",
    "            else:\n",
    "                url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+str(number_of_months)+\"01.zip\"\n",
    "            \n",
    "            \n",
    "            url_list.append(url)\n",
    "            number_of_months=number_of_months+1\n",
    "\n",
    "        return self.maybe_download(url_list,year)\n",
    "        \n",
    "    def fetch_year(self):\n",
    "        year = input('Enter the year for which you need to fetch the log files: ')\n",
    "        self.create_directory(\"Part_2_log_datasets_trial/\"+year+\"/\")\n",
    "        log.basicConfig(filename='Part_2_log_datasets_trial/EDGAR_LogFileDataset_LogFile.log', level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "        \n",
    "        #fetch the year for which the user wants logs\n",
    "       \n",
    "        log.info('Start of program')\n",
    "        year=int(year)\n",
    "        if(year >= 2003 and year < 2016):\n",
    "            #calling the function to generate dynamic URL\n",
    "            return self.generate_url(year)\n",
    "        else:\n",
    "            print(\"EDGAR log files are available for years 2003-2016. Kindly enter a year within this range\")\n",
    "            fetch_year()\n",
    "    \n",
    "                \n",
    "    def create_zip_folder(self,path):\n",
    "        zipfolder_name=path+'.zip'\n",
    "        zf = zipfile.ZipFile(zipfolder_name, \"w\")\n",
    "        for dirname, subdirs, files in os.walk(path):\n",
    "            zf.write(dirname)\n",
    "            for filename in files:\n",
    "                zf.write(os.path.join(dirname, filename))\n",
    "        zf.close()\n",
    "    \n",
    "        \n",
    "        \n",
    "    def upload_zip_to_s3(self,filetoupload):\n",
    "        print(\"Upload to s3\")\n",
    "        S3_ACCESS_KEY= input(\"Enter S3_ACCESS_KEY : \")\n",
    "        S3_SECRET_KEY =  input(\"Enter S3_SECRET_KEY : \")\n",
    "        \n",
    "\n",
    "        try:\n",
    "            conn = tinys3.Connection(S3_ACCESS_KEY,S3_SECRET_KEY)\n",
    "            bucket = input(\"Enter BUCKET_NAME : \")\n",
    "            f = open(filetoupload,'rb')\n",
    "#             print(\"this is f\",f)\n",
    "#             print(\"this is file to upload\",filetoupload)\n",
    "#             print(\"this is bucket\",bucket)\n",
    "            conn.upload(filetoupload,f,bucket)\n",
    "            print(\"Upload to s3 successfull\")\n",
    "           \n",
    "        except Exception:\n",
    "            print(\"INVALID keys, please try again\")\n",
    "            self.upload_zip_to_s3(filetoupload)\n",
    "           \n",
    "        \n",
    "get_data_obj=GetData()\n",
    "merged_dataframe=get_data_obj.fetch_year()\n",
    "#fetch the year for which the user wants logs\n",
    "#year = input('Enter the year for which you need to fetch the log files: ')\n",
    "#calling the function to generate dynamic URL\n",
    "\n",
    "#df=get_data_obj.generate_url(year)\n",
    "\n",
    "\n",
    "class Process_and_analyse_data():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "    \n",
    "    def format_dataframe_columns(self):\n",
    "        #convert all the integer column in int format\n",
    "        log.info(\"Data fetched, started cleaning\")\n",
    "        print(\"Data fetched, started cleaning\")\n",
    "        df['zone'] = df['zone'].astype('int')\n",
    "        df['cik'] = df['cik'].astype('int')\n",
    "        df['code'] = df['code'].astype('int')\n",
    "        df['idx']=df['idx'].astype('int')\n",
    "        df['norefer']=df['norefer'].astype('int')\n",
    "        df['noagent']=df['noagent'].astype('int')\n",
    "        df['find']=df['find'].astype('int')\n",
    "        df['crawler']=df['crawler'].astype('int')\n",
    "        \n",
    "        #replacing empty strings with NaN \n",
    "        df.replace(r'\\s+', np.nan, regex=True)\n",
    "        log.info(\"Formatted the columns of the dataframe\")\n",
    "        print(\"Formatted the columns of the dataframe\")\n",
    "        self.handle_nan_values()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def handle_nan_values(self):\n",
    "        \n",
    "        #replace all ip column NaN value by a default ip address \n",
    "        df[\"ip\"].fillna(\"255.255.255.255\", inplace=True)\n",
    "\n",
    "        #perform forward fill to replace NaN values by fetching the next valid value\n",
    "        df[\"date\"].fillna(method='ffill')\n",
    "\n",
    "        #perform backward fill to replace NaN values by backpropagating and fetching the previous valid value\n",
    "        df[\"time\"].fillna(method='bfill')\n",
    "\n",
    "        #replace all zone column NaN values by 'Not Available' extension\n",
    "        df[\"zone\"].fillna(\"Not Available\", inplace=True)\n",
    "\n",
    "        #replace all extension column NaN values by default extension\n",
    "        df[\"extention\"].fillna(\"-index.htm\", inplace=True)\n",
    "\n",
    "        #replace all size column NaN values by 0 and convert the column into integer \n",
    "        df[\"size\"].fillna(0, inplace=True)\n",
    "        df['size'] = df['size'].astype('int')\n",
    "\n",
    "        #replace all user agent column NaN values by the default value 1 (no user agent)\n",
    "        df[\"noagent\"].fillna(\"Not Applicable\", inplace=True)\n",
    "\n",
    "        #replace all find column NaN values by the default value 0 (no character strings found)\n",
    "        df[\"find\"].fillna(0, inplace=True)\n",
    "\n",
    "        #replace all broser column NaN values by a string\n",
    "        df[\"browser\"].fillna(\"Not Available\", inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if the value in idx column is missing, check the value of the extension column, if its \"-index.html\" set the column's value 1 else 0\n",
    "        count=0\n",
    "        for i in df['idx']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df['extension'][count]==\"-index.htm\"):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            count=count+1\n",
    "\n",
    "        # if the value of norefer column is missing, check the value of the find column, if it is 0, set the value 1, else it set it 0\n",
    "        counter=0\n",
    "        for i in df['norefer']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df[\"find\"][counter]==0):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            counter=counter+1\n",
    "\n",
    "        # if the value of crawler is missing, check the value of the code, if it is 404 set it as 1 else 0\n",
    "        count_position=0\n",
    "        for i in df['crawler']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df[\"code\"][count_position]==404):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            count_position=count_position+1\n",
    "        log.info(\"Replacing NaN values with appropriate replacement\")\n",
    "        log.info(\"Handling missing values completed\")\n",
    "        print(\"Handling missing values completed\")\n",
    "        \n",
    "        log.info(\"Exporting merged dataframe to local system\")\n",
    "        print(\"Exporting merged dataframe to local system\")\n",
    "        df.to_csv(\"Part_2_log_datasets_trial/merged_dataframe.csv\")\n",
    "        log.info(\"Merged dataframe exported\")\n",
    "        print(\"Merged dataframe exported\")\n",
    "        \n",
    "        merged_dataframe\n",
    "        self.identify_cik_accession_number_anomaly()\n",
    "    \n",
    "        \n",
    "    def identify_cik_accession_number_anomaly(self):\n",
    "        #this operation requires a large amount of time for computaton, thus we are performing this on a subset of data\n",
    "        small_df=df.head(25)\n",
    "        #insert a column to check CIK, Accession number discripancy\n",
    "        small_df.insert(6, \"CIK_Accession_Anamoly_Flag\", \"N\")\n",
    "                \n",
    "        #check if CIK and Accession number match. The Accession number is divided into three parts, CIK-Year-Number_of_filings_listed.\n",
    "        #the first part i.e the CIK must match with the CIK column. If not, there exists an anomaly\n",
    "\n",
    "        count=0;\n",
    "        print(\"Creating CIK_Accession_Anomaly_Flag column to check anomaly\")\n",
    "        log.info(\"Creating CIK_Accession_Anomaly_Flag column to check anomaly\")\n",
    "        \n",
    "        for i in small_df['accession']:\n",
    "            #fetch the CIK number from the accession number and convert it into integer\n",
    "            list_of_fetched_cik_from_accession=[(int(i.split(\"-\")[0]))]\n",
    "\n",
    "            #check if the CIK number from the column and CIK number fetched from the accession number are equal\n",
    "            if(small_df['cik'][count]!=list_of_fetched_cik_from_accession):\n",
    "                small_df['CIK_Accession_Anamoly_Flag'][count]=\"Y\"\n",
    "\n",
    "            count=count+1\n",
    "        log.info(\"CIK Accession Anomaly flag computed\")\n",
    "        print(\"CIK Accession Anomaly flag computed\")\n",
    "        print(small_df)\n",
    "        self.get_file_name_from_extension()\n",
    "        \n",
    "    def get_file_name_from_extension(self):\n",
    "        #this operation requires a large amount of time for computaton, thus we are performing this on a subset of data\n",
    "        small_df=df.head(25)\n",
    "        small_df.insert(7, \"filename\", \"\")\n",
    "        print(\"Creating filename column\")\n",
    "        log.info(\"Creating filename column\")\n",
    "        #Extension rule: if the file name is missing and only the file extension is present, then the file name is document accession number\n",
    "        count=0\n",
    "        for i in small_df[\"extention\"]:\n",
    "            if(i==\".txt\"):\n",
    "                # if the value in extension is only .txt, fetch the accession number and append accession number to .txt\n",
    "                #list_of_fetched_cik_from_accession=int(((df2[\"accession\"].str.split(\"-\")[count])[0]))\n",
    "                #print((df[\"accession\"]).astype(str))\n",
    "                #list_of_fetched_cik_from_accession=int(df[\"accession\"])\n",
    "                small_df[\"filename\"][count]=(small_df[\"accession\"][count])+\".txt\" \n",
    "            else:\n",
    "                small_df[\"filename\"][count]=i\n",
    "            count=count+1\n",
    "        print(\"Filename column created\")\n",
    "        print(small_df)\n",
    "        log.info(\"Filename column created\")\n",
    "        \n",
    "get_data_obj=GetData()\n",
    "df=get_data_obj.getDataFrame()\n",
    "df_list=get_data_obj.getDataFrameList()\n",
    "process_data_obj=Process_and_analyse_data()\n",
    "process_data_obj.format_dataframe_columns()\n",
    "\n",
    "log.info(\"Zipping the folder for loading in S3\")\n",
    "get_data_obj.create_zip_folder(\"Part_2_log_datasets_trial\")\n",
    "get_data_obj.upload_zip_to_s3(\"Part_2_log_datasets_trial.zip\")\n",
    "log.info(\"Data zipped and loaded on S3\")\n",
    "print(\"Data zipped and loaded on S3\")\n",
    "log.info(\"Pipeline completed!!\")\n",
    "log.info(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Starting Analysis\")\n",
    "combined_df = pd.read_csv(\"Part_2_log_datasets_trial/merged_dataframe.csv\") #  pass your 12 month combined csv here\n",
    "# group by cik and date and get count of ciks for a date   \n",
    "temp_df=combined_df.groupby(['cik','date'])['cik'].count()\n",
    "temp_df.head()\n",
    "\n",
    "# convert group by result into a frame\n",
    "\n",
    "grouped_frame = pd.DataFrame(temp_df.reset_index(name = \"hit_count\"))\n",
    "\n",
    "print(\"Grouping by CIK and Date : \")\n",
    "print(grouped_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Monitor change in hit count\n",
    "\n",
    "def get_percent_change(curr, prev):\n",
    "        change_in_perc = ((curr - prev)/prev ) * 100\n",
    "        return change_in_perc\n",
    "\n",
    "count = 0\n",
    "analysis_df = pd.DataFrame()\n",
    "frame_count = 0\n",
    "for row in grouped_frame['cik']:\n",
    "    current_cik = grouped_frame['cik'][count]\n",
    "    current_hit_count = grouped_frame['hit_count'][count]\n",
    "    current_date = grouped_frame['date'][count]\n",
    "    if(count >= 1):\n",
    "        if(current_cik == grouped_frame['cik'][count-1]):\n",
    "            change_in_count = current_hit_count - grouped_frame['hit_count'][count-1] \n",
    "            change_in_perc = get_percent_change(current_hit_count,grouped_frame['hit_count'][count-1])\n",
    "            \n",
    "            if(change_in_perc >= 1000 ): ## decide on threshold\n",
    "                analysis_df.loc[frame_count, 'cik'] = current_cik\n",
    "                analysis_df.loc[frame_count, 'date'] = current_date\n",
    "                analysis_df.loc[frame_count, 'change in %'] = change_in_perc\n",
    "                frame_count += 1\n",
    "                #print(current_cik ,\" changed by\",change_in_perc,\" % on \",current_date)\n",
    "                \n",
    "    count +=1\n",
    "    \n",
    "analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \n",
    "# Load the data into a DataFrame\n",
    "data = pd.read_csv('Part_2_log_datasets_trial/merged_dataframe.csv') # pass your single month stuff here\n",
    "#grouping by IP\n",
    "byIp = data.groupby('ip')\n",
    "byIp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "byCIK = data.groupby('cik')\n",
    "byCIK['size'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#getting requests with  status code 404\n",
    "byIp404=data[data['code']==404]\n",
    "byIp404\n",
    "#Anamoly-request with 404 has a download size associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "byIp404=data[data['code']==404].groupby('ip')\n",
    "byIp404['size'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1. Simple describe function on data\n",
    "summary = data.describe()\n",
    "summary\n",
    "#Analysis: total number of requests on this day-  261289\n",
    "#Average request per day :215\n",
    "#Max file download size:5.259544e+07\n",
    "#Avergae File donwload size:\t1.703470e+05"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
