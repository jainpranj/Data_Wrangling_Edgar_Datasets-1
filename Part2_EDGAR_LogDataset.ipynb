{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the year for which you need to fetch the log files: 2015\n",
      "Downloading data\n",
      "Download complete\n",
      "log20160101.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vasanti\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2881: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ip        date      time  zone        cik             accession  \\\n",
      "0  100.43.81.bbb  2016-01-01  00:00:00   0.0   857737.0  0001564590-15-010715   \n",
      "1  100.43.85.jbj  2016-01-01  00:00:00   0.0   910322.0  0000899140-15-000782   \n",
      "2  101.81.76.eib  2016-01-01  00:00:00   0.0   744822.0  0000878467-15-000834   \n",
      "3  101.81.76.eib  2016-01-01  00:00:00   0.0  1403802.0  0001393905-15-000679   \n",
      "4  101.81.76.eib  2016-01-01  00:00:00   0.0  1199070.0  0001127602-15-035138   \n",
      "\n",
      "               extention   code     size  idx  norefer  noagent  find  \\\n",
      "0  icon-20150930_cal.xml  200.0   5194.0  0.0      0.0      0.0  10.0   \n",
      "1          b15605421.htm  200.0  11840.0  0.0      0.0      0.0  10.0   \n",
      "2             -index.htm  200.0  67184.0  1.0      0.0      0.0  10.0   \n",
      "3             -index.htm  200.0   7019.0  1.0      0.0      0.0  10.0   \n",
      "4             -index.htm  200.0   6743.0  1.0      0.0      0.0  10.0   \n",
      "\n",
      "   crawler browser  \n",
      "0      0.0     NaN  \n",
      "1      0.0     NaN  \n",
      "2      0.0     NaN  \n",
      "3      0.0     NaN  \n",
      "4      0.0     NaN  \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "def generate_url(year):\n",
    "    \n",
    "    #generate the url for fetching the log files for every month's first day\n",
    "    number_of_months=1\n",
    "    while number_of_months < 13:\n",
    "        if(number_of_months <10):\n",
    "            url=\"http://www.sec.gov/dera/data/PublicEDGAR-log-file-data/\"+year+\"/Qtr1/log\"+year+'%02d' % number_of_months+\"01.zip\"\n",
    "        else:\n",
    "            url=\"http://www.sec.gov/dera/data/PublicEDGAR-log-file-data/\"+year+\"/Qtr1/log\"+year+str(number_of_months)+\"01.zip\"\n",
    "        number_of_months=number_of_months+1\n",
    "    #temp_url=download_data(\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/2016/Qtr1/log20160101.zip\")\n",
    "    maybe_download(\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/2016/Qtr1/log20160101.zip\")\n",
    "    \n",
    "def maybe_download(url):\n",
    "    \n",
    "    import urllib.request\n",
    "    import requests\n",
    "    import os.path\n",
    "    import zipfile\n",
    "    import pandas as pd\n",
    "   \n",
    "    #fetching the zip file name from the URL\n",
    "    file_name=url.split(\"/\")\n",
    "    \n",
    "    #Downloading data if not already present in the cache\n",
    "    if(os.path.exists(\"Part_2_log_datasets/\"+file_name[8])):\n",
    "        print(\"Already present\")\n",
    "        \n",
    "    else:\n",
    "        urllib.request.urlretrieve(url, \"Part_2_log_datasets/\"+file_name[8])\n",
    "        print(\"Download complete\")\n",
    "        \n",
    "    #unzip the file and fetch the csv file\n",
    "    zf = zipfile.ZipFile(\"Part_2_log_datasets/\"+file_name[8]) \n",
    "    csv_file_name=file_name[8].replace(\"zip\", \"csv\")\n",
    "    zf_file=zf.open(csv_file_name)\n",
    "    \n",
    "    #create a dataframe from the csv\n",
    "    df = pd.read_csv(zf_file)\n",
    "  \n",
    "    \n",
    "def trial_download(url):\n",
    "    import zipfile\n",
    "    from zipfile import ZipFile\n",
    "    import urllib.request\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "    url = urllib.request.urlopen(url)\n",
    "    with ZipFile(BytesIO(url.read())) as my_zip_file:\n",
    "        pd.read\n",
    "        for contained_file in my_zip_file.namelist():\n",
    "            print(\"unzipping maybe\")\n",
    "    \n",
    "def download_data(temp_url):\n",
    "    print(\"In download URL function\")\n",
    "    import pandas as pd\n",
    "    import zipfile\n",
    "    zf = zipfile.ZipFile(temp_url)\n",
    "    zf_file=zf.open('log20030101.zip')\n",
    "    df = pd.read_csv(zf_file)\n",
    "    #df = pd.read_csv(url, compression='zip', header=0, sep=',', quotechar='\"')\n",
    "    print(df.head(5))\n",
    "    #you need to install request package using $ pip install requests\n",
    "    \"\"\"from io import BytesIO\n",
    "    from zipfile import ZipFile\n",
    "    import urllib.request\n",
    "    import csv\n",
    "    url = urllib.request.urlopen(url)\n",
    "       \n",
    "    with ZipFile(BytesIO(url.read())) as my_zip_file:\n",
    "        for contained_file in my_zip_file.namelist():\n",
    "            print(\"unzipping maybe\")\n",
    "            uncompress_size = sum((file.file_size for file in my_zip_file.infolist()))\n",
    "\n",
    "            extracted_size = 0\n",
    "\n",
    "            for file in my_zip_file.infolist():\n",
    "                extracted_size += file.file_size\n",
    "                print (extracted_size * 100/uncompress_size)\n",
    "                my_zip_file.extract(file)\n",
    "                \n",
    "            # with open((\"unzipped_and_read_\" + contained_file + \".file\"), \"wb\") as output:\n",
    "            \n",
    "            for line in my_zip_file.open(contained_file).readlines():\n",
    "                print(\"reading line one by one started\")\n",
    "                line=[line]\n",
    "                with open('test.csv', 'w', newline='') as fp:\n",
    "                    a = csv.writer(fp, delimiter=',')\n",
    "                    a.writerows(line)\n",
    "            print(\"Done\")\"\"\"\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #fetch the year for which the user wants logs\n",
    "    year = input('Enter the year for which you need to fetch the log files: ')\n",
    "    #calling the function to generate dynamic URL\n",
    "    generate_url(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
