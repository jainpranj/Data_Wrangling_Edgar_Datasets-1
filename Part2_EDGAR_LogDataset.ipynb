{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import urllib3 as ur\n",
    "import urllib.request as ur\n",
    "import os.path\n",
    "import zipfile\n",
    "import tinys3\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import logging as log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the year (eg : 2003) for which you need to fetch the log files. Note: Data available for years 2003 through 2016 only.2003\n",
      "Generating the URL's for the year\n",
      "Downloading data for all the months\n",
      "Data for  log20030101.zip not present in cache. Downloading data\n",
      "Data for  log20030201.zip not present in cache. Downloading data\n",
      "Data for  log20030301.zip not present in cache. Downloading data\n",
      "Data for  log20030401.zip not present in cache. Downloading data\n",
      "Data for  log20030501.zip not present in cache. Downloading data\n",
      "Data for  log20030601.zip not present in cache. Downloading data\n",
      "Data for  log20030701.zip not present in cache. Downloading data\n",
      "Data for  log20030801.zip not present in cache. Downloading data\n",
      "Data for  log20030901.zip not present in cache. Downloading data\n",
      "Data for  log20031001.zip not present in cache. Downloading data\n",
      "Data for  log20031101.zip not present in cache. Downloading data\n",
      "Data for  log20031201.zip not present in cache. Downloading data\n",
      "All the files are downloaded and unzipped\n",
      "Creating a dataframe from the csv and appending it to the list of dataframe\n",
      "Data fetched, started cleaning\n",
      "Formatted the columns of the dataframe\n",
      "Handling missing values completed\n",
      "Exporting merged dataframe to local system\n",
      "Merged dataframe exported\n",
      "Creating CIK_Accession_Anomaly_Flag column to check anomaly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:309: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:287: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIK Accession Anomaly flag computed\n",
      "                 ip        date      time  zone      cik  \\\n",
      "0    129.110.39.jca  2003-03-01  00:00:00   500    97349   \n",
      "1     61.115.76.jbf  2003-03-01  00:00:00   500   766351   \n",
      "2     61.115.76.jbf  2003-03-01  00:00:01   500   902584   \n",
      "3     61.115.76.jbf  2003-03-01  00:00:03   500   778207   \n",
      "4    129.110.39.jca  2003-03-01  00:00:07   500    97349   \n",
      "5     208.62.55.eib  2003-03-01  00:00:07   500    56824   \n",
      "6    129.110.39.jca  2003-03-01  00:00:10   500    97349   \n",
      "7     208.62.55.eib  2003-03-01  00:00:12   500    56824   \n",
      "8    129.110.39.jca  2003-03-01  00:00:13   500    97349   \n",
      "9   148.139.130.hhi  2003-03-01  00:00:16   500  1108205   \n",
      "10   129.110.39.jca  2003-03-01  00:00:17   500    97349   \n",
      "11   129.110.39.jca  2003-03-01  00:00:19   500    97349   \n",
      "12    61.115.76.jbf  2003-03-01  00:00:21   500   778207   \n",
      "13    67.81.137.eci  2003-03-01  00:00:21   500   857323   \n",
      "14   203.200.34.ejd  2003-03-01  00:00:31   500  1125051   \n",
      "15   129.110.39.jca  2003-03-01  00:00:32   500    97349   \n",
      "16  148.139.130.hhi  2003-03-01  00:00:33   500  1108205   \n",
      "17   203.200.34.ejd  2003-03-01  00:00:34   500  1125051   \n",
      "18    67.81.137.eci  2003-03-01  00:00:43   500   857323   \n",
      "19   202.65.158.iae  2003-03-01  00:00:44   500   276999   \n",
      "20  205.188.208.dcg  2003-03-01  00:01:01   500  1059784   \n",
      "21    24.26.233.ede  2003-03-01  00:01:07   500    56824   \n",
      "22    67.81.137.eci  2003-03-01  00:01:13   500   857323   \n",
      "23    68.73.147.ede  2003-03-01  00:01:25   500    30554   \n",
      "24    67.81.137.eci  2003-03-01  00:01:27   500   857323   \n",
      "\n",
      "               accession CIK_Accession_Anamoly_Flag          extention  code  \\\n",
      "0   0000097349-01-000006                          N          -0002.txt   200   \n",
      "1   0000950134-03-003149                          Y               .txt   200   \n",
      "2   0000902584-03-000044                          N               .txt   200   \n",
      "3   9999999997-03-006003                          Y               .txt   200   \n",
      "4   0000097349-01-000006                          N         -index.htm   200   \n",
      "5   0000950124-03-000077                          Y         -index.htm   200   \n",
      "6   0000097349-01-000006                          N          -0003.txt   200   \n",
      "7   0000950124-03-000077                          Y  k73883e10vqza.txt   200   \n",
      "8   0000097349-01-000006                          N         -index.htm   200   \n",
      "9   0000927016-02-005853                          Y         -index.htm   304   \n",
      "10  0000097349-01-000006                          N          -0003.txt   200   \n",
      "11  0000097349-01-000006                          N         -index.htm   200   \n",
      "12  0000950144-03-002432                          Y               .txt   200   \n",
      "13  0000908662-03-000043                          Y       form_8-k.txt   200   \n",
      "14  0001002014-03-000092                          Y         -index.htm   200   \n",
      "15  0000097349-01-000006                          N          -0004.txt   200   \n",
      "16  0000927016-02-005853                          Y            d8k.txt   304   \n",
      "17  0001002014-03-000092                          Y       form3wai.htm   200   \n",
      "18  0000908662-03-000043                          Y        exh_99a.txt   200   \n",
      "19  0000912057-00-037191                          Y               .txt   200   \n",
      "20  0000950116-03-001778                          Y         def14a.txt   200   \n",
      "21  0000902664-03-000355                          Y         -index.htm   304   \n",
      "22  0000950123-03-000015                          Y         -index.htm   200   \n",
      "23  0000030554-02-000014                          N         -index.htm   200   \n",
      "24  0000950123-03-000015                          Y    y67235e10vq.txt   200   \n",
      "\n",
      "       size  idx  norefer  noagent  find  crawler        browser  \n",
      "0      3726    0        0        0     9        0            win  \n",
      "1    995957    0        1        0     0        0  Not Available  \n",
      "2     15520    0        1        0     0        0  Not Available  \n",
      "3      1670    0        1        0     0        0  Not Available  \n",
      "4      4331    1        0        0     1        0            win  \n",
      "5      2727    1        0        0     1        0            win  \n",
      "6      1211    0        0        0     9        0            win  \n",
      "7    158260    0        0        0     9        0            win  \n",
      "8      4331    1        0        0     1        0            win  \n",
      "9         0    1        0        0     1        0            win  \n",
      "10     1211    0        0        0     9        0            win  \n",
      "11     4331    1        0        0     1        0            win  \n",
      "12  1974869    0        1        0     0        0  Not Available  \n",
      "13     2510    0        0        0     9        0            win  \n",
      "14     2832    1        0        0     4        0            win  \n",
      "15   316657    0        0        0     9        0            win  \n",
      "16        0    0        0        0     9        0            win  \n",
      "17    22217    0        0        0     9        0            win  \n",
      "18    11813    0        0        0     9        0            win  \n",
      "19    46393    0        0        0     4        0            win  \n",
      "20    82625    0        1        0     0        0            mie  \n",
      "21        0    1        0        0     1        0            mie  \n",
      "22     2819    1        0        0     1        0            win  \n",
      "23     1977    1        0        0     4        0            mie  \n",
      "24   161884    0        0        0     9        0            win  \n",
      "Creating filename column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:333: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:331: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename column created\n",
      "                 ip        date      time  zone      cik  \\\n",
      "0    129.110.39.jca  2003-03-01  00:00:00   500    97349   \n",
      "1     61.115.76.jbf  2003-03-01  00:00:00   500   766351   \n",
      "2     61.115.76.jbf  2003-03-01  00:00:01   500   902584   \n",
      "3     61.115.76.jbf  2003-03-01  00:00:03   500   778207   \n",
      "4    129.110.39.jca  2003-03-01  00:00:07   500    97349   \n",
      "5     208.62.55.eib  2003-03-01  00:00:07   500    56824   \n",
      "6    129.110.39.jca  2003-03-01  00:00:10   500    97349   \n",
      "7     208.62.55.eib  2003-03-01  00:00:12   500    56824   \n",
      "8    129.110.39.jca  2003-03-01  00:00:13   500    97349   \n",
      "9   148.139.130.hhi  2003-03-01  00:00:16   500  1108205   \n",
      "10   129.110.39.jca  2003-03-01  00:00:17   500    97349   \n",
      "11   129.110.39.jca  2003-03-01  00:00:19   500    97349   \n",
      "12    61.115.76.jbf  2003-03-01  00:00:21   500   778207   \n",
      "13    67.81.137.eci  2003-03-01  00:00:21   500   857323   \n",
      "14   203.200.34.ejd  2003-03-01  00:00:31   500  1125051   \n",
      "15   129.110.39.jca  2003-03-01  00:00:32   500    97349   \n",
      "16  148.139.130.hhi  2003-03-01  00:00:33   500  1108205   \n",
      "17   203.200.34.ejd  2003-03-01  00:00:34   500  1125051   \n",
      "18    67.81.137.eci  2003-03-01  00:00:43   500   857323   \n",
      "19   202.65.158.iae  2003-03-01  00:00:44   500   276999   \n",
      "20  205.188.208.dcg  2003-03-01  00:01:01   500  1059784   \n",
      "21    24.26.233.ede  2003-03-01  00:01:07   500    56824   \n",
      "22    67.81.137.eci  2003-03-01  00:01:13   500   857323   \n",
      "23    68.73.147.ede  2003-03-01  00:01:25   500    30554   \n",
      "24    67.81.137.eci  2003-03-01  00:01:27   500   857323   \n",
      "\n",
      "               accession          extention                  filename  code  \\\n",
      "0   0000097349-01-000006          -0002.txt                 -0002.txt   200   \n",
      "1   0000950134-03-003149               .txt  0000950134-03-003149.txt   200   \n",
      "2   0000902584-03-000044               .txt  0000902584-03-000044.txt   200   \n",
      "3   9999999997-03-006003               .txt  9999999997-03-006003.txt   200   \n",
      "4   0000097349-01-000006         -index.htm                -index.htm   200   \n",
      "5   0000950124-03-000077         -index.htm                -index.htm   200   \n",
      "6   0000097349-01-000006          -0003.txt                 -0003.txt   200   \n",
      "7   0000950124-03-000077  k73883e10vqza.txt         k73883e10vqza.txt   200   \n",
      "8   0000097349-01-000006         -index.htm                -index.htm   200   \n",
      "9   0000927016-02-005853         -index.htm                -index.htm   304   \n",
      "10  0000097349-01-000006          -0003.txt                 -0003.txt   200   \n",
      "11  0000097349-01-000006         -index.htm                -index.htm   200   \n",
      "12  0000950144-03-002432               .txt  0000950144-03-002432.txt   200   \n",
      "13  0000908662-03-000043       form_8-k.txt              form_8-k.txt   200   \n",
      "14  0001002014-03-000092         -index.htm                -index.htm   200   \n",
      "15  0000097349-01-000006          -0004.txt                 -0004.txt   200   \n",
      "16  0000927016-02-005853            d8k.txt                   d8k.txt   304   \n",
      "17  0001002014-03-000092       form3wai.htm              form3wai.htm   200   \n",
      "18  0000908662-03-000043        exh_99a.txt               exh_99a.txt   200   \n",
      "19  0000912057-00-037191               .txt  0000912057-00-037191.txt   200   \n",
      "20  0000950116-03-001778         def14a.txt                def14a.txt   200   \n",
      "21  0000902664-03-000355         -index.htm                -index.htm   304   \n",
      "22  0000950123-03-000015         -index.htm                -index.htm   200   \n",
      "23  0000030554-02-000014         -index.htm                -index.htm   200   \n",
      "24  0000950123-03-000015    y67235e10vq.txt           y67235e10vq.txt   200   \n",
      "\n",
      "       size  idx  norefer  noagent  find  crawler        browser  \n",
      "0      3726    0        0        0     9        0            win  \n",
      "1    995957    0        1        0     0        0  Not Available  \n",
      "2     15520    0        1        0     0        0  Not Available  \n",
      "3      1670    0        1        0     0        0  Not Available  \n",
      "4      4331    1        0        0     1        0            win  \n",
      "5      2727    1        0        0     1        0            win  \n",
      "6      1211    0        0        0     9        0            win  \n",
      "7    158260    0        0        0     9        0            win  \n",
      "8      4331    1        0        0     1        0            win  \n",
      "9         0    1        0        0     1        0            win  \n",
      "10     1211    0        0        0     9        0            win  \n",
      "11     4331    1        0        0     1        0            win  \n",
      "12  1974869    0        1        0     0        0  Not Available  \n",
      "13     2510    0        0        0     9        0            win  \n",
      "14     2832    1        0        0     4        0            win  \n",
      "15   316657    0        0        0     9        0            win  \n",
      "16        0    0        0        0     9        0            win  \n",
      "17    22217    0        0        0     9        0            win  \n",
      "18    11813    0        0        0     9        0            win  \n",
      "19    46393    0        0        0     4        0            win  \n",
      "20    82625    0        1        0     0        0            mie  \n",
      "21        0    1        0        0     1        0            mie  \n",
      "22     2819    1        0        0     1        0            win  \n",
      "23     1977    1        0        0     4        0            mie  \n",
      "24   161884    0        0        0     9        0            win  \n",
      "Upload to s3\n",
      "Enter S3_ACCESS_KEY : dfs\n",
      "Enter S3_SECRET_KEY : sdfsd\n",
      "Enter BUCKET_NAME : sdf\n",
      "INVALID keys\n",
      "Proceed without uploading to s3? Y/N : (Select N to try again)Y\n",
      "Folder not uploaded to S3. Proceeding to Analysis \n",
      "Data zipped and loaded on S3\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python       \n",
    "merged_dataframe=pd.DataFrame()\n",
    "df_list_global=list()\n",
    "class GetData:\n",
    "   \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "    def create_directory(self,path):\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "    \n",
    "    def setDataFrame(self, df):\n",
    "        merged_dataframe = df\n",
    "        \n",
    "    def getDataFrame(self):\n",
    "        return merged_dataframe\n",
    "    \n",
    "    def setDataFrameList(self, list_of_df):\n",
    "        \n",
    "        df_list_global = list_of_df\n",
    "      \n",
    "        \n",
    "    def getDataFrameList(self):\n",
    "        return df_list_global\n",
    "    \n",
    "    def maybe_download(self, url_list, year):\n",
    "        \n",
    "        df_list=['df1','df2','df3','df4','df5','df6','df7','df8','df9','df10','df11','df12']\n",
    "        year=str(year)\n",
    "        count=0\n",
    "        print(\"Downloading data for all the months\")\n",
    "        log.info(\"Downloading data for all the months\")\n",
    "        \n",
    "        for i in url_list:\n",
    "\n",
    "            #fetching the zip file name from the URL\n",
    "            file_name=i.split(\"/\")\n",
    "           \n",
    "\n",
    "            #Downloading data if not already present in the cache\n",
    "            if(os.path.exists(\"Part_2_log_datasets/\"+year+\"/\"+file_name[8])):\n",
    "                print(\"Data for \",file_name[8],\" is already present, pulling it from cache\")\n",
    "                \n",
    "\n",
    "            else:\n",
    "                #pbar = ProgressBar(widgets=[Percentage(), Bar()])\n",
    "                ur.urlretrieve(i, \"Part_2_log_datasets/\"+year+\"/\"+file_name[8])\n",
    "                #ur.urlretrieve(i, \"Part_2_log_datasets/\"+year+\"/\"+file_name[8], reporthook)\n",
    "                print(\"Data for \",file_name[8],\"not present in cache. Downloading data\")\n",
    "                \n",
    "            \n",
    "            #unzip the file and fetch the csv file\n",
    "            zf = zipfile.ZipFile(\"Part_2_log_datasets/\"+year+\"/\"+file_name[8]) \n",
    "            csv_file_name=file_name[8].replace(\"zip\", \"csv\")\n",
    "            zf_file=zf.open(csv_file_name)\n",
    "            \n",
    "            \n",
    "            #create a dataframe from the csv and append it to the list of dataframe\n",
    "            df_list[count]=pd.read_csv(zf_file)\n",
    "           \n",
    "            count=count+1 \n",
    "        \n",
    "        print(\"All the files are downloaded and unzipped\")\n",
    "        log.info(\"All the files are downloaded and unzipped\")\n",
    "        print(\"Creating a dataframe from the csv and appending it to the list of dataframe\")\n",
    "        log.info(\"Creating a dataframe from the csv and appending it to the list of dataframe\")\n",
    "        \n",
    "        self.setDataFrameList(df_list)\n",
    "        log.info(\"Merging the dataframe\")\n",
    "        #merging the data into one dataframe\n",
    "        merged_dataframe=pd.concat([df_list[0],df_list[1],df_list[2],df_list[3],df_list[4],df_list[5],df_list[6],df_list[7],df_list[8],df_list[9],df_list[10],df_list[11]], ignore_index=True)\n",
    "        self.setDataFrame(merged_dataframe)\n",
    "        return merged_dataframe\n",
    "    \n",
    "\n",
    "    def generate_url(self, year):\n",
    "        log.info('In generate URL method')\n",
    "        print(\"Generating the URL's for the year\")\n",
    "        log.info(\"Generating the URL's for the year\")\n",
    "        url_list=list()\n",
    "        #generate the url for fetching the log files for every month's first day\n",
    "        number_of_months=1\n",
    "\n",
    "        while number_of_months < 13:\n",
    "            #find the quarter for the month\n",
    "            if number_of_months >= 1 and number_of_months < 4:\n",
    "                quarter=\"Qtr1\"\n",
    "            elif(number_of_months >= 4 and number_of_months < 7):\n",
    "                quarter=\"Qtr2\"\n",
    "            elif(number_of_months >= 7 and number_of_months < 10):\n",
    "                quarter=\"Qtr3\"\n",
    "            elif(number_of_months >= 10 and number_of_months < 13):\n",
    "                quarter=\"Qtr4\"\n",
    "\n",
    "            if(number_of_months <10):\n",
    "                url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+'%02d' % number_of_months+\"01.zip\"\n",
    "\n",
    "            else:\n",
    "                url=\"http://www.sec.gov/dera/data/Public-EDGAR-log-file-data/\"+str(year)+\"/\"+quarter+\"/log\"+str(year)+str(number_of_months)+\"01.zip\"\n",
    "            \n",
    "            \n",
    "            url_list.append(url)\n",
    "            number_of_months=number_of_months+1\n",
    "\n",
    "        return self.maybe_download(url_list,year)\n",
    "        \n",
    "    def fetch_year(self):\n",
    "        year1 = input('Enter the year (eg : 2003) for which you need to fetch the log files. Note: Data available for years 2003 through 2016 only.')\n",
    "\n",
    "        try:\n",
    "            year=int(year1)\n",
    "            if(year >= 2003 and year <= 2016):\n",
    "                #calling the function to generate dynamic URL\n",
    "                self.create_directory(\"Part_2_log_datasets/\"+str(year)+\"/\")\n",
    "                log.basicConfig(filename='Part_2_log_datasets/EDGAR_LogFileDataset_LogFile.log', level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "                return self.generate_url(year)\n",
    "            else:\n",
    "                print(\"EDGAR log files are available for years 2003-2016. Kindly enter a year within this range\")\n",
    "                self.fetch_year()\n",
    "                \n",
    "        except Exception:\n",
    "            print(\"Invalid input. Please try again\")\n",
    "            self.fetch_year()\n",
    "        #fetch the year for which the user wants logs\n",
    "       \n",
    "        log.info('Start of program')    \n",
    "    \n",
    "                \n",
    "    def create_zip_folder(self,path):\n",
    "        zipfolder_name=path+'.zip'\n",
    "        zf = zipfile.ZipFile(zipfolder_name, \"w\")\n",
    "        for dirname, subdirs, files in os.walk(path):\n",
    "            zf.write(dirname)\n",
    "            for filename in files:\n",
    "                zf.write(os.path.join(dirname, filename))\n",
    "        zf.close()\n",
    "    \n",
    "        \n",
    "        \n",
    "    def upload_zip_to_s3(self,filetoupload):\n",
    "        print(\"Upload to s3\")\n",
    "        S3_ACCESS_KEY= input(\"Enter S3_ACCESS_KEY : \")\n",
    "        S3_SECRET_KEY =  input(\"Enter S3_SECRET_KEY : \")\n",
    "        \n",
    "\n",
    "        try:\n",
    "            conn = tinys3.Connection(S3_ACCESS_KEY,S3_SECRET_KEY)\n",
    "            bucket = input(\"Enter BUCKET_NAME : \")\n",
    "            f = open(filetoupload,'rb')\n",
    "#             print(\"this is f\",f)\n",
    "#             print(\"this is file to upload\",filetoupload)\n",
    "#             print(\"this is bucket\",bucket)\n",
    "            conn.upload(filetoupload,f,bucket)\n",
    "            log.info(\"Data zipped and loaded on S3\")\n",
    "            print(\"Upload to s3 successfull. Proceeding to Analysis\")\n",
    "           \n",
    "        except Exception:\n",
    "            print(\"INVALID keys\")\n",
    "            choice = input(\"Proceed without uploading to s3? Y/N : (Select N to try again)\")\n",
    "            if(choice == \"Y\" or choice == \"y\"):\n",
    "                print(\"Folder not uploaded to S3. Proceeding to Analysis \")\n",
    "\n",
    "            elif(choice == \"N\" or choice == \"n\"):\n",
    "                self.upload_zip_to_s3(filetoupload)\n",
    "                \n",
    "            else:\n",
    "                print(\"Invalid input. Try again.\")\n",
    "                self.upload_zip_to_s3(filetoupload)\n",
    "                \n",
    "           \n",
    "        \n",
    "get_data_obj=GetData()\n",
    "merged_dataframe=get_data_obj.fetch_year()\n",
    "#fetch the year for which the user wants logs\n",
    "#year = input('Enter the year for which you need to fetch the log files: ')\n",
    "#calling the function to generate dynamic URL\n",
    "\n",
    "#df=get_data_obj.generate_url(year)\n",
    "\n",
    "\n",
    "class Process_and_analyse_data():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Retrieves and stores the urllib.urlopen object for a given url\n",
    "        \"\"\"\n",
    "    \n",
    "    def format_dataframe_columns(self):\n",
    "        #convert all the integer column in int format\n",
    "        log.info(\"Data fetched, started cleaning\")\n",
    "        print(\"Data fetched, started cleaning\")\n",
    "        df['zone'] = df['zone'].astype('int')\n",
    "        df['cik'] = df['cik'].astype('int')\n",
    "        df['code'] = df['code'].astype('int')\n",
    "        df['idx']=df['idx'].astype('int')\n",
    "        df['norefer']=df['norefer'].astype('int')\n",
    "        df['noagent']=df['noagent'].astype('int')\n",
    "        df['find']=df['find'].astype('int')\n",
    "        df['crawler']=df['crawler'].astype('int')\n",
    "        \n",
    "        #replacing empty strings with NaN \n",
    "        df.replace(r'\\s+', np.nan, regex=True)\n",
    "        log.info(\"Formatted the columns of the dataframe\")\n",
    "        print(\"Formatted the columns of the dataframe\")\n",
    "        self.handle_nan_values()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def handle_nan_values(self):\n",
    "        \n",
    "        #replace all ip column NaN value by a default ip address \n",
    "        df[\"ip\"].fillna(\"255.255.255.255\", inplace=True)\n",
    "\n",
    "        #perform forward fill to replace NaN values by fetching the next valid value\n",
    "        df[\"date\"].fillna(method='ffill')\n",
    "\n",
    "        #perform backward fill to replace NaN values by backpropagating and fetching the previous valid value\n",
    "        df[\"time\"].fillna(method='bfill')\n",
    "\n",
    "        #replace all zone column NaN values by 'Not Available' extension\n",
    "        df[\"zone\"].fillna(\"Not Available\", inplace=True)\n",
    "\n",
    "        #replace all extension column NaN values by default extension\n",
    "        df[\"extention\"].fillna(\"-index.htm\", inplace=True)\n",
    "\n",
    "        #replace all size column NaN values by 0 and convert the column into integer \n",
    "        df[\"size\"].fillna(0, inplace=True)\n",
    "        df['size'] = df['size'].astype('int')\n",
    "\n",
    "        #replace all user agent column NaN values by the default value 1 (no user agent)\n",
    "        df[\"noagent\"].fillna(\"Not Applicable\", inplace=True)\n",
    "\n",
    "        #replace all find column NaN values by the default value 0 (no character strings found)\n",
    "        df[\"find\"].fillna(0, inplace=True)\n",
    "\n",
    "        #replace all broser column NaN values by a string\n",
    "        df[\"browser\"].fillna(\"Not Available\", inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if the value in idx column is missing, check the value of the extension column, if its \"-index.html\" set the column's value 1 else 0\n",
    "        count=0\n",
    "        for i in df['idx']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df['extension'][count]==\"-index.htm\"):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            count=count+1\n",
    "\n",
    "        # if the value of norefer column is missing, check the value of the find column, if it is 0, set the value 1, else it set it 0\n",
    "        counter=0\n",
    "        for i in df['norefer']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df[\"find\"][counter]==0):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            counter=counter+1\n",
    "\n",
    "        # if the value of crawler is missing, check the value of the code, if it is 404 set it as 1 else 0\n",
    "        count_position=0\n",
    "        for i in df['crawler']:\n",
    "            if(np.isnan(i)):\n",
    "                if(df[\"code\"][count_position]==404):\n",
    "                    i=1\n",
    "                else:\n",
    "                    i=0\n",
    "            count_position=count_position+1\n",
    "        log.info(\"Replacing NaN values with appropriate replacement\")\n",
    "        log.info(\"Handling missing values completed\")\n",
    "        print(\"Handling missing values completed\")\n",
    "        \n",
    "        log.info(\"Exporting merged dataframe to local system\")\n",
    "        print(\"Exporting merged dataframe to local system\")\n",
    "        df.to_csv(\"Part_2_log_datasets/merged_dataframe.csv\")\n",
    "        log.info(\"Merged dataframe exported\")\n",
    "        print(\"Merged dataframe exported\")\n",
    "        \n",
    "        merged_dataframe\n",
    "        self.identify_cik_accession_number_anomaly()\n",
    "    \n",
    "        \n",
    "    def identify_cik_accession_number_anomaly(self):\n",
    "        #this operation requires a large amount of time for computaton, thus we are performing this on a subset of data\n",
    "        small_df=df.head(25)\n",
    "        #insert a column to check CIK, Accession number discripancy\n",
    "        small_df.insert(6, \"CIK_Accession_Anamoly_Flag\", \"N\")\n",
    "                \n",
    "        #check if CIK and Accession number match. The Accession number is divided into three parts, CIK-Year-Number_of_filings_listed.\n",
    "        #the first part i.e the CIK must match with the CIK column. If not, there exists an anomaly\n",
    "\n",
    "        count=0;\n",
    "        print(\"Creating CIK_Accession_Anomaly_Flag column to check anomaly\")\n",
    "        log.info(\"Creating CIK_Accession_Anomaly_Flag column to check anomaly\")\n",
    "        \n",
    "        for i in small_df['accession']:\n",
    "            #fetch the CIK number from the accession number and convert it into integer\n",
    "            list_of_fetched_cik_from_accession=[(int(i.split(\"-\")[0]))]\n",
    "\n",
    "            #check if the CIK number from the column and CIK number fetched from the accession number are equal\n",
    "            if(small_df['cik'][count]!=list_of_fetched_cik_from_accession):\n",
    "                small_df['CIK_Accession_Anamoly_Flag'][count]=\"Y\"\n",
    "\n",
    "            count=count+1\n",
    "        log.info(\"CIK Accession Anomaly flag computed\")\n",
    "        print(\"CIK Accession Anomaly flag computed\")\n",
    "        print(small_df)\n",
    "        self.get_file_name_from_extension()\n",
    "        \n",
    "    def get_file_name_from_extension(self):\n",
    "        #this operation requires a large amount of time for computaton, thus we are performing this on a subset of data\n",
    "        small_df=df.head(25)\n",
    "        small_df.insert(7, \"filename\", \"\")\n",
    "        print(\"Creating filename column\")\n",
    "        log.info(\"Creating filename column\")\n",
    "        #Extension rule: if the file name is missing and only the file extension is present, then the file name is document accession number\n",
    "        count=0\n",
    "        for i in small_df[\"extention\"]:\n",
    "            if(i==\".txt\"):\n",
    "                # if the value in extension is only .txt, fetch the accession number and append accession number to .txt\n",
    "                #list_of_fetched_cik_from_accession=int(((df2[\"accession\"].str.split(\"-\")[count])[0]))\n",
    "                #print((df[\"accession\"]).astype(str))\n",
    "                #list_of_fetched_cik_from_accession=int(df[\"accession\"])\n",
    "                small_df[\"filename\"][count]=(small_df[\"accession\"][count])+\".txt\" \n",
    "            else:\n",
    "                small_df[\"filename\"][count]=i\n",
    "            count=count+1\n",
    "        print(\"Filename column created\")\n",
    "        print(small_df)\n",
    "        log.info(\"Filename column created\")\n",
    "        \n",
    "get_data_obj=GetData()\n",
    "df=get_data_obj.getDataFrame()\n",
    "df_list=get_data_obj.getDataFrameList()\n",
    "process_data_obj=Process_and_analyse_data()\n",
    "process_data_obj.format_dataframe_columns()\n",
    "\n",
    "log.info(\"Zipping the folder for loading in S3\")\n",
    "get_data_obj.create_zip_folder(\"Part_2_log_datasets\")\n",
    "get_data_obj.upload_zip_to_s3(\"Part_2_log_datasets.zip\")\n",
    "\n",
    "\n",
    "log.info(\"Pipeline completed!!\")\n",
    "log.info(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Starting Analysis\")\n",
    "combined_df = pd.read_csv(\"Part_2_log_datasets/merged_dataframe.csv\") #  pass your 12 month combined csv here\n",
    "# group by cik and date and get count of ciks for a date   \n",
    "temp_df=combined_df.groupby(['cik','date'])['cik'].count()\n",
    "temp_df.head()\n",
    "\n",
    "# convert group by result into a frame\n",
    "\n",
    "grouped_frame = pd.DataFrame(temp_df.reset_index(name = \"hit_count\"))\n",
    "\n",
    "print(\"Grouping by CIK and Date and getting count for each CIK  \")\n",
    "print(grouped_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Monitor change in hit count\n",
    "# get no of ips(hit count) for CIK per month(only day 1 representing a monthhas been used)\n",
    "# if the change is more than 1000 % select that CIK and make a new dataframe for it\n",
    "print(\"Monitoring the change in hit count\")\n",
    "def get_percent_change(curr, prev):\n",
    "        change_in_perc = ((curr - prev)/prev ) * 100\n",
    "        return change_in_perc\n",
    "\n",
    "count = 0\n",
    "analysis_df = pd.DataFrame()\n",
    "frame_count = 0\n",
    "for row in grouped_frame['cik']:\n",
    "    current_cik = grouped_frame['cik'][count]\n",
    "    current_hit_count = grouped_frame['hit_count'][count]\n",
    "    current_date = grouped_frame['date'][count]\n",
    "    if(count >= 1):\n",
    "        if(current_cik == grouped_frame['cik'][count-1]):\n",
    "            change_in_count = current_hit_count - grouped_frame['hit_count'][count-1] \n",
    "            change_in_perc = get_percent_change(current_hit_count,grouped_frame['hit_count'][count-1])\n",
    "            \n",
    "            if(change_in_perc >= 1000 ): ## decide on threshold\n",
    "                analysis_df.loc[frame_count, 'cik'] = current_cik\n",
    "                analysis_df.loc[frame_count, 'date'] = current_date\n",
    "                analysis_df.loc[frame_count, 'current count'] = current_hit_count\n",
    "                analysis_df.loc[frame_count, 'previous count'] = grouped_frame['cik'][count-1]\n",
    "                analysis_df.loc[frame_count, 'change in %'] = change_in_perc\n",
    "                frame_count += 1\n",
    "                #print(current_cik ,\" changed by\",change_in_perc,\" % on \",current_date)\n",
    "                \n",
    "    count +=1\n",
    "    \n",
    "print(analysis_df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
